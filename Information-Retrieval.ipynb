{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6639fbd4-241f-47dc-8106-2267da55392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION! \n",
    "# The following code may need to be run once before using this module.\n",
    "# If needed, uncomment the code below, run cell a single time, and then comment out again.\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75215b5-bbb0-4b24-8281-8f3abcf5caf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search a user specified number of random Wikipedia pages for a user query:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Specify the number of Wikipedia pages to search:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Please be patient. The Wikipedia pages retrieval may take a few minutes to complete. *\n",
      "Sourcing Pages: 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,\n",
      "Retrieving Pages: 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,\n",
      "Cleaning Text: 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Provide your search query here (submit 'q' to quit):\n",
      "\t military munitions weapon army navy air force marines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1: https://en.wikipedia.org/wiki/7-Mile_Drome\n",
      " 2: https://en.wikipedia.org/wiki/Pneumatisation\n",
      " 3: https://en.wikipedia.org/wiki/Lt._Gen._Ricardo_Sanchez\n",
      " 4: https://en.wikipedia.org/wiki/Xhezair_Shaqiri\n",
      " 5: https://en.wikipedia.org/wiki/ZSYW\n",
      " 6: https://en.wikipedia.org/wiki/Iw\n",
      " 7: https://en.wikipedia.org/wiki/IV_Aquitanorum_eq_c.R.\n",
      " 8: https://en.wikipedia.org/wiki/X-4_missile\n",
      " 9: https://en.wikipedia.org/wiki/HRH_Prince_Michael_of_Kent\n",
      "10: https://en.wikipedia.org/wiki/Gbara\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Provide your search query here (submit 'q' to quit):\n",
      "\t radio music news sports weather\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1: https://en.wikipedia.org/wiki/KQEV-LP\n",
      " 2: https://en.wikipedia.org/wiki/KSDC-LP\n",
      " 3: https://en.wikipedia.org/wiki/UPI\n",
      " 4: https://en.wikipedia.org/wiki/KWBP_(FM)\n",
      " 5: https://en.wikipedia.org/wiki/KEDA_(AM)\n",
      " 6: https://en.wikipedia.org/wiki/Wdt\n",
      " 7: https://en.wikipedia.org/wiki/Dsc\n",
      " 8: https://en.wikipedia.org/wiki/UH-University_Park\n",
      " 9: https://en.wikipedia.org/wiki/N%27Gasobil\n",
      "10: https://en.wikipedia.org/wiki/Ni\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Provide your search query here (submit 'q' to quit):\n",
      "\t q\n"
     ]
    }
   ],
   "source": [
    "# ATTENTION! \n",
    "# The following code will save txt files in the form 'wiki-page-#.txt' to the current directory.\n",
    "\n",
    "import random\n",
    "import heapq\n",
    "import requests\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "wiki_site = 'https://en.wikipedia.org'\n",
    "wiki_index = '/wiki/Wikipedia:Contents/A–Z_index'\n",
    "project_page = '/wiki/Special:AllPages/'\n",
    "special_page = '/wiki/'\n",
    "save_file = 'wiki-page'\n",
    "num_urls = 10\n",
    "\n",
    "\n",
    "# Search a user specified number of random Wikipedia pages for a user specified query.\n",
    "def main():\n",
    "    print(\"Search a user specified number of random Wikipedia pages for a user query:\\n\")\n",
    "    num_pages = get_num_pages()\n",
    "    page_links = source_wiki_pages(num_pages)\n",
    "    wiki_pages = retrieve_wiki_pages(page_links)\n",
    "    clean_save_text(wiki_pages)\n",
    "    perform_search(*compile_corpus(num_pages))\n",
    "    \n",
    "    \n",
    "# Requests a user specified number of Wikipedia pages to include in the search.\n",
    "def get_num_pages():\n",
    "    while True:\n",
    "        try:\n",
    "            num_pages = abs(int(input(\"Specify the number of Wikipedia pages to search: \").strip()))\n",
    "            break\n",
    "        except:\n",
    "            print(\"* Your input must be a positive integer. *\")\n",
    "            \n",
    "    return num_pages\n",
    "            \n",
    "            \n",
    "# Sources the Wikipedia pages to be retrieved below.\n",
    "def source_wiki_pages(num_pages):\n",
    "    count = 0\n",
    "    page_links = []\n",
    "    print(\"\\n* Please be patient. The Wikipedia pages retrieval may take a few minutes to complete. *\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            sub_indices = get_wiki_link(num_pages, project_page, wiki_index)\n",
    "            print(\"Sourcing Pages: \", end='')\n",
    "            \n",
    "            for sub_index in sub_indices:\n",
    "                count += 1\n",
    "                print(f\"{count},\", end='')\n",
    "                page_link = get_wiki_link(1, special_page+sub_index[22:], sub_index)[0]\n",
    "                page_links.append(page_link)\n",
    "            break\n",
    "        except:\n",
    "            print(\"\\n*** Experienced an error. Trying again. ***\")\n",
    "    print()\n",
    "    \n",
    "    return page_links\n",
    "\n",
    "\n",
    "# Retrieves the sourced Wikipedia pages identified above.\n",
    "def retrieve_wiki_pages(page_links):\n",
    "    count = 0\n",
    "    wiki_pages = {}\n",
    "    print(\"Retrieving Pages: \", end='')\n",
    "    \n",
    "    for page_link in page_links:\n",
    "        count += 1\n",
    "        \n",
    "        try:\n",
    "            wiki_pages[f\"{wiki_site}{page_link}\"] = request_wiki_page(page_link)\n",
    "            print(f\"{count},\", end='')\n",
    "        except:\n",
    "            print(f\"\\n**** An exception occurred. Skipping page {count}. ****\")\n",
    "    print()\n",
    "    \n",
    "    return wiki_pages\n",
    "\n",
    "\n",
    "# Saves to disk the url and text from the Wikipedia pages retrieved above.\n",
    "def clean_save_text(wiki_pages):\n",
    "    count = 0\n",
    "    print(\"Cleaning Text: \", end='')\n",
    "    \n",
    "    for page_url, page_text in wiki_pages.items():\n",
    "        count += 1\n",
    "        print(f\"{count},\", end='')\n",
    "        cleaned_text = clean_text(page_text)\n",
    "        save_to_file(count, page_url, cleaned_text)\n",
    "        \n",
    "        \n",
    "# Tokenizes and cleans text discarding stopwords, punctuation, adjectives, etc.\n",
    "def clean_text(text):\n",
    "    cleaned_text = []\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = pos_tag(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token[0].lower() not in stopwords.words('english'):\n",
    "            if token[0] not in punctuation and token[0] not in [\"''\",'``','–',\"'s\",\"'t\",\"'m\",\"'d\",\"'ve\",\"'re\",\"'ll\",'•','·']:\n",
    "                if token[0].lower().find('wiki') == -1 and token[1] not in ['JJ', 'JJR', 'JJS']:\n",
    "                    cleaned_text.append(token[0].lower().replace(\"'\", ''))\n",
    "                            \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Retrieves a Wikipedia index page and parses the content for appropriate links using BeautifulSoup\n",
    "def get_wiki_link(num_pages, page_type, sub_page):\n",
    "    wiki_links = []\n",
    "    url = f\"{wiki_site}{sub_page}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    all_links = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "\n",
    "    for _ in range(num_pages):\n",
    "        random.shuffle(all_links)\n",
    "\n",
    "        for link in all_links:\n",
    "            if link['href'].find(page_type) == -1:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        wiki_links.append(link['href'])\n",
    "\n",
    "    return wiki_links\n",
    "\n",
    "\n",
    "# Retrieves a Wikipedia page and parses the content for text using BeautifulSoup.\n",
    "def request_wiki_page(wiki_link):\n",
    "    url = f\"{wiki_site}{wiki_link}\"\n",
    "    page_request = requests.get(url)\n",
    "    page_text = BeautifulSoup(page_request.text, features='lxml').get_text()\n",
    "    \n",
    "    return page_text\n",
    "\n",
    "\n",
    "# Saves a file in the form 'wiki-page#.txt' to the current directory including the url and text.\n",
    "def save_to_file(num, page_url, wiki_text):\n",
    "    file_name = f\"{save_file}-{num}.txt\"\n",
    "\n",
    "    with open(file_name, 'w') as filehandle:\n",
    "        filehandle.write(f\"{page_url}\\n\")\n",
    "\n",
    "        for text in wiki_text:\n",
    "            try:\n",
    "                filehandle.write(f\"{text}\\n\")\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "                \n",
    "# Opens a saved 'wiki-page#.txt' file from the current directory and returns the url and text.\n",
    "def open_saved_file(num):\n",
    "    wiki_text = []\n",
    "    file_name = f\"{save_file}-{num}.txt\"\n",
    "    \n",
    "    with open(file_name, 'r') as filehandle:\n",
    "        url = filehandle.readline().strip()\n",
    "        line = filehandle.readline()\n",
    "\n",
    "        while line:\n",
    "            wiki_text.append(line)\n",
    "            line = filehandle.readline().strip()\n",
    "\n",
    "    return url, wiki_text\n",
    "\n",
    "\n",
    "# The following performs a search within the saved radom Wikipedia pages for the terms provided by the user.\n",
    "# Creates a model using gensim.TfidfModel and uses the model to vectorize the text.\n",
    "# Stores the vector scores in a nested dict for each term in user query for each Wikipedia page.\n",
    "# Sums all the vector scores for each Wikipedia page and places the total score in a priority queue.\n",
    "# Pulls and displays up to 10 of the most relevent Wikepedia page links.\n",
    "def compile_corpus(num_pages):\n",
    "    urls = []\n",
    "    data = []\n",
    "    \n",
    "    for num in range(num_pages):\n",
    "        url, datum = open_saved_file(num+1)\n",
    "        urls.append(url)\n",
    "        data.append(datum)\n",
    "\n",
    "    dictionary = Dictionary(data)\n",
    "    unique_words = dictionary.token2id.keys()\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in data]\n",
    "    model = TfidfModel(corpus)\n",
    "    \n",
    "    return dictionary, unique_words, corpus, model, urls\n",
    "    \n",
    "    \n",
    "def perform_search(*args):\n",
    "    url_scores = []\n",
    "    corpus_scores = {}\n",
    "    count = 1\n",
    "    num = num_urls\n",
    "    dictionary, unique_words, corpus, model, urls = args\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nProvide your search query here (submit 'q' to quit):\\n\\t\")\n",
    "            if query == 'q' or query == 'Q':\n",
    "                return\n",
    "            terms = clean_text(query)\n",
    "            break\n",
    "        except:\n",
    "            print(\"\\n** Something went wrong. Please try again. **\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        corpus_scores[urls[i]] = {}\n",
    "        vector = model[corpus[i]]\n",
    "        \n",
    "        for term in terms:\n",
    "            if term in unique_words:\n",
    "                term_id = dictionary.token2id[term]\n",
    "                corpus_scores[urls[i]].update({term: 0})\n",
    "                \n",
    "                for word_id, score in vector:\n",
    "                    if word_id == term_id:\n",
    "                        corpus_scores[urls[i]][term] = score\n",
    "                        break\n",
    "    \n",
    "    for url, terms_scores in corpus_scores.items():\n",
    "        total_score = 0\n",
    "        \n",
    "        for term, score in terms_scores.items():\n",
    "            if score > 0:\n",
    "                total_score += score\n",
    "                    \n",
    "        heapq.heappush(url_scores, (0-total_score, url))\n",
    "        \n",
    "    while len(url_scores) > 0 and num > 0:\n",
    "        temp = heapq.heappop(url_scores)\n",
    "        print(f\"{count:>2}: {temp[1]}\")\n",
    "        count += 1\n",
    "        num -= 1\n",
    "        \n",
    "    perform_search(*args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37475ff-9a56-4fc5-8e47-93618f618321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
